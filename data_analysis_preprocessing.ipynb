{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "# Data Cleaning, Preprocessing and Analysis\n",
    "\n",
    "## 1)Data Cleaning\n",
    "\n",
    "### Check for 'Null' or 'NaN'\n",
    "\n",
    "# TeST LINE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_dir = \"./data\"\n",
    "\n",
    "# 1. Load all CSVs from the directory and concatenate\n",
    "dfs = []\n",
    "for fname in os.listdir(csv_dir):\n",
    "    if fname.endswith(\".csv\"):\n",
    "        path = os.path.join(csv_dir, fname)\n",
    "        dfs.append(pd.read_csv(path))\n",
    "\n",
    "df = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "print(\"Columns in raw data:\", df.columns.tolist())\n",
    "\n",
    "# 2. Define attributes (all columns)\n",
    "attributes = list(df.columns)\n",
    "\n",
    "# 3. Convert data columns to numeric\n",
    "for col in attributes:\n",
    "    df[col] = pd.to_numeric(df[col], errors=\"coerce\")\n",
    "\n",
    "# 4. Global sanity checks (only on attribute columns)\n",
    "n_nulls = df[attributes].isnull().sum().sum()\n",
    "n_nans = df[attributes].isna().sum().sum()\n",
    "non_numeric_cols = df[attributes].select_dtypes(exclude=[\"number\"]).columns.tolist()\n",
    "\n",
    "print(f\"Total Nulls found: {n_nulls}\")\n",
    "print(f\"Total NaNs found:  {n_nans}\")\n",
    "print(f\"Non-numeric columns: {non_numeric_cols}\")\n",
    "\n",
    "# 5. Per-column summary\n",
    "print(\"\\nPer-column nulls/NaNs:\")\n",
    "print(df[attributes].isna().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "### Checking for the max/min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "attr_names = attributes\n",
    "nums = df[attr_names]\n",
    "\n",
    "print(f\"{'Attribute':25} {'Min':>10} {'Max':>10}\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "for col in attr_names:\n",
    "    vals = nums[col]\n",
    "    print(f\"{col:25} {vals.min():10.3f} {vals.max():10.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "### Checking negative/zero/positive values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_dir = \"./data\"\n",
    "\n",
    "# Load and concatenate all CSVs\n",
    "dfs = []\n",
    "for fname in os.listdir(csv_dir):\n",
    "    if fname.endswith(\".csv\"):\n",
    "        path = os.path.join(csv_dir, fname)\n",
    "        dfs.append(pd.read_csv(path))\n",
    "\n",
    "df = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "zero = (nums == 0).sum()\n",
    "pos  = (nums > 0).sum()\n",
    "neg  = (nums < 0).sum()\n",
    "\n",
    "x = np.arange(len(attributes))\n",
    "w = 0.25\n",
    "\n",
    "plt.figure(figsize=(16, 6))\n",
    "plt.bar(x - w, zero.values, w, label=\"zeros\")\n",
    "plt.bar(x,     pos.values,  w, label=\"positive\")\n",
    "plt.bar(x + w, neg.values,  w, label=\"negative\")\n",
    "\n",
    "plt.xticks(x, attributes, rotation=45, ha=\"right\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.title(\"Zeros / positive / negative per attribute (all CSVs in ./data)\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "looks all good except of total_et, lets see if the negative values arise just from one dataframe, or in more\n",
    "\n",
    "### Checking for negative/zero/positive values for 'total_et' for the individual loaded location files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_dir = \"./data\"\n",
    "\n",
    "df_labels = []\n",
    "neg_counts = []\n",
    "zero_counts = []\n",
    "pos_counts = []\n",
    "\n",
    "# Loop over all CSVs in ./data\n",
    "for fname in sorted(os.listdir(csv_dir)):\n",
    "    if not fname.endswith(\".csv\"):\n",
    "        continue\n",
    "\n",
    "    path = os.path.join(csv_dir, fname)\n",
    "    df = pd.read_csv(path)\n",
    "\n",
    "    # Make sure total_et is numeric\n",
    "    te = pd.to_numeric(df[\"total_et\"], errors=\"coerce\")\n",
    "\n",
    "    df_labels.append(fname)  # or fname.replace(\".csv\", \"\") if you prefer\n",
    "    neg_counts.append((te < 0).sum())\n",
    "    zero_counts.append((te == 0).sum())\n",
    "    pos_counts.append((te > 0).sum())\n",
    "\n",
    "# Convert to numpy arrays\n",
    "neg_counts = np.array(neg_counts)\n",
    "zero_counts = np.array(zero_counts)\n",
    "pos_counts = np.array(pos_counts)\n",
    "\n",
    "# Plot: three bars (neg/zero/pos) per CSV\n",
    "x = np.arange(len(df_labels))\n",
    "width = 0.25\n",
    "\n",
    "plt.figure(figsize=(18, 6))\n",
    "plt.bar(x - width, neg_counts,  width, label=\"negative\")\n",
    "plt.bar(x,         zero_counts, width, label=\"zero\")\n",
    "plt.bar(x + width, pos_counts,  width, label=\"positive\")\n",
    "\n",
    "plt.ylabel(\"Count (total_et)\")\n",
    "plt.title(\"total_et distribution per CSV (negative / zero / positive)\")\n",
    "plt.xticks(x, df_labels, rotation=90)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "Here is a question mark:\n",
    "Physically, ET should be ≥ 0 (it’s a water flux leaving the surface). Negative values mean either:\n",
    "different sign convention (e.g. positive = downward flux, negative = upward), or the variable is actually net surface flux mislabeled as “ET”, or errors in the data / preprocessing.\n",
    "\n",
    "Zeros themselves (no ET on some days) are not crazy — e.g. frozen soil, snow cover, very cold/dry air — but the many negative values are suspicious if the variable is truly “total evapotranspiration (mm)”."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "## 2) Data Analysis\n",
    "\n",
    "### Average Attribute Correlation across all 100 locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_dir = \"./data\"\n",
    "\n",
    "corr_matrices = []\n",
    "valid_files = []\n",
    "\n",
    "for fname in os.listdir(csv_dir):\n",
    "    if not fname.endswith(\".csv\"):\n",
    "        continue\n",
    "\n",
    "    path = os.path.join(csv_dir, fname)\n",
    "    df = pd.read_csv(path)\n",
    "\n",
    "    # numeric columns only\n",
    "    df_numeric = df.select_dtypes(include=\"number\")\n",
    "\n",
    "    if df_numeric.shape[1] < 2:\n",
    "        continue\n",
    "\n",
    "    corr = df_numeric.corr()\n",
    "    corr_matrices.append(corr)\n",
    "    valid_files.append(fname)\n",
    "\n",
    "if not corr_matrices:\n",
    "    print(\"No valid CSVs found for correlation.\")\n",
    "else:\n",
    "    # union of all columns\n",
    "    all_cols = sorted(set().union(*[c.index for c in corr_matrices]))\n",
    "\n",
    "    # reindex each corr matrix to same shape\n",
    "    corr_reindexed = [\n",
    "        c.reindex(index=all_cols, columns=all_cols)\n",
    "        for c in corr_matrices\n",
    "    ]\n",
    "\n",
    "    # average, ignoring NaNs\n",
    "    avg_corr = np.nanmean(np.stack(corr_reindexed), axis=0)\n",
    "    avg_corr_df = pd.DataFrame(avg_corr, index=all_cols, columns=all_cols)\n",
    "\n",
    "    print(f\"Average correlation matrix from {len(valid_files)} CSVs\")\n",
    "\n",
    "    # ---------- Heatmap ----------\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    im = plt.imshow(avg_corr_df.values, aspect='auto')\n",
    "    plt.colorbar(im, label=\"Correlation\")\n",
    "\n",
    "    plt.xticks(ticks=np.arange(len(all_cols)), labels=all_cols, rotation=90)\n",
    "    plt.yticks(ticks=np.arange(len(all_cols)), labels=all_cols)\n",
    "\n",
    "    plt.title(\"Average correlation heatmap across all basins\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "## 3) Data Preprocessing for Prediction\n",
    "\n",
    "### Add target colums to csv file for predicting 'prec' 1, 3, 7 day(s) ahead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dir = \"./data\"\n",
    "output_dir = \"./data_for_prediction\"\n",
    "\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "target_cols = [\"prec_1d_ahead\", \"prec_3d_ahead\", \"prec_7d_ahead\"]\n",
    "\n",
    "files_with_missing = []\n",
    "\n",
    "for fname in os.listdir(input_dir):\n",
    "    if not fname.endswith(\".csv\"):\n",
    "        continue\n",
    "\n",
    "    in_path = os.path.join(input_dir, fname)\n",
    "    out_path = os.path.join(output_dir, fname)\n",
    "\n",
    "    df = pd.read_csv(in_path)\n",
    "\n",
    "    # Ensure prec is numeric\n",
    "    df[\"prec\"] = pd.to_numeric(df[\"prec\"], errors=\"coerce\")\n",
    "\n",
    "    # Sort by date if possible\n",
    "    if {\"YYYY\", \"MM\", \"DD\"}.issubset(df.columns):\n",
    "        df = df.sort_values([\"YYYY\", \"MM\", \"DD\"]).reset_index(drop=True)\n",
    "\n",
    "    # Create future target columns per basin using shift\n",
    "    df[\"prec_1d_ahead\"] = df[\"prec\"].shift(-1)\n",
    "    df[\"prec_3d_ahead\"] = df[\"prec\"].shift(-3)\n",
    "    df[\"prec_7d_ahead\"] = df[\"prec\"].shift(-7)\n",
    "\n",
    "    # Forward-fill missing targets (e.g. at the end after shift)\n",
    "    df[target_cols] = df[target_cols].ffill()\n",
    "\n",
    "    # Sanity check: only on target columns\n",
    "    n_missing = df[target_cols].isna().sum().sum()\n",
    "    if n_missing > 0:\n",
    "        files_with_missing.append((fname, int(n_missing)))\n",
    "\n",
    "    # Save processed file\n",
    "    df.to_csv(out_path, index=False)\n",
    "\n",
    "# ---- Single summary message at the end ----\n",
    "if files_with_missing:\n",
    "    print(\"⚠️ Some files still have NaNs in target columns:\")\n",
    "    for fname, n in files_with_missing:\n",
    "        print(f\"  - {fname}: {n} missing values in {target_cols}\")\n",
    "else:\n",
    "    print(\"✅ All files processed: no NaNs in target columns\", target_cols)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
